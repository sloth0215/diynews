"""
ë¸”ë¡œê·¸ Fetcher (ë„¤ì´ë²„, í‹°ìŠ¤í† ë¦¬, Medium ë“±)
RSS ê¸°ë°˜ ë¸”ë¡œê·¸ í”Œëž«í¼ ì§€ì›
"""

import re
import feedparser
from datetime import datetime
from dateutil import parser as date_parser
from fetchers.base_fetcher import BaseFetcher


class BlogFetcher(BaseFetcher):
    """ë¸”ë¡œê·¸ RSS Fetcher"""
    
    def can_handle(self, url: str) -> bool:
        """ë¸”ë¡œê·¸ URLì¸ì§€ í™•ì¸"""
        blog_domains = [
            'blog.naver.com',
            'tistory.com',
            'medium.com',
            'brunch.co.kr',
            'velog.io'
        ]
        return any(domain in url for domain in blog_domains)
    
    def convert_to_rss_url(self, url: str) -> str:
        """
        ë¸”ë¡œê·¸ URLì„ RSS í”¼ë“œ URLë¡œ ë³€í™˜
        
        Args:
            url (str): ì›ë³¸ URL
            
        Returns:
            str: RSS í”¼ë“œ URL
        """
        # ì´ë¯¸ RSS URLì´ë©´ ê·¸ëŒ€ë¡œ
        if '/rss' in url.lower() or url.endswith('.xml'):
            return url
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸
        if 'blog.naver.com' in url:
            match = re.search(r'blog\.naver\.com/([^/\?]+)', url)
            if match:
                blog_id = match.group(1)
                rss_url = f"https://rss.blog.naver.com/{blog_id}.xml"
                print(f"ðŸ”„ ë„¤ì´ë²„ ë¸”ë¡œê·¸ RSS: {rss_url}")
                return rss_url
        
        # í‹°ìŠ¤í† ë¦¬
        elif 'tistory.com' in url:
            base_url = url.rstrip('/')
            if not base_url.endswith('/rss'):
                rss_url = f"{base_url}/rss"
                print(f"ðŸ”„ í‹°ìŠ¤í† ë¦¬ RSS: {rss_url}")
                return rss_url
        
        # Medium
        elif 'medium.com' in url:
            if '/@' in url:
                rss_url = url.replace('medium.com/', 'medium.com/feed/')
                print(f"ðŸ”„ Medium RSS: {rss_url}")
                return rss_url
        
        # Velog
        elif 'velog.io' in url:
            match = re.search(r'velog\.io/@([^/\?]+)', url)
            if match:
                username = match.group(1)
                rss_url = f"https://v2.velog.io/rss/@{username}"
                print(f"ðŸ”„ Velog RSS: {rss_url}")
                return rss_url
        
        print(f"â„¹ï¸  RSS ìžë™ ë³€í™˜ ë¶ˆê°€: {url}")
        return url
    
    def fetch_feed(self, url: str) -> list:
        """
        ë¸”ë¡œê·¸ RSS í”¼ë“œ ìˆ˜ì§‘
        
        Args:
            url (str): ë¸”ë¡œê·¸ URL
            
        Returns:
            list: ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # RSS URL ë³€í™˜
            rss_url = self.convert_to_rss_url(url)
            print(f"ðŸ” í”¼ë“œ ìˆ˜ì§‘ ì¤‘: {rss_url}")
            
            # RSS íŒŒì‹±
            feed = feedparser.parse(rss_url)
            
            if feed.bozo:
                print(f"âš ï¸  í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
            
            if not feed.entries:
                print(f"âŒ ê²Œì‹œë¬¼ ì—†ìŒ: {rss_url}")
                return []
            
            # ê²Œì‹œë¬¼ í•„í„°ë§ (ìµœê·¼ Nê°œë§Œ ì²˜ë¦¬)
            posts = []
            count = 0
            for entry in feed.entries:
                if count >= self.max_entries:
                    print(f"â„¹ï¸  ìµœëŒ€ {self.max_entries}ê°œ ë„ë‹¬, ë‚˜ë¨¸ì§€ ìƒëžµ")
                    break
                    
                post = self._parse_entry(entry)
                if post and self._is_recent(post):
                    posts.append(post)
                    count += 1
                elif post:
                    # ë‚ ì§œê°€ 7ì¼ ì´ì „ì´ë©´ ì¤‘ë‹¨ (RSSëŠ” ìµœì‹ ìˆœì´ë¯€ë¡œ)
                    print(f"â„¹ï¸  7ì¼ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
            
            print(f"âœ… {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")
            return posts
            
        except Exception as e:
            print(f"âŒ í”¼ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
            return []
    
    def _parse_entry(self, entry) -> dict:
        """
        RSS ì—”íŠ¸ë¦¬ë¥¼ ê²Œì‹œë¬¼ ë°ì´í„°ë¡œ ë³€í™˜
        
        Args:
            entry: feedparser entry ê°ì²´
            
        Returns:
            dict: ê²Œì‹œë¬¼ ë°ì´í„°
        """
        try:
            post = {
                'title': entry.get('title', 'ì œëª© ì—†ìŒ'),
                'url': entry.get('link', ''),
                'content': self._extract_content(entry),
                'published': self._extract_date(entry),
                'thumbnail': self._extract_thumbnail(entry)
            }
            return post
        except Exception as e:
            print(f"âš ï¸  ì—”íŠ¸ë¦¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_content(self, entry) -> str:
        """ê²Œì‹œë¬¼ ë‚´ìš© ì¶”ì¶œ"""
        if hasattr(entry, 'content') and entry.content:
            return entry.content[0].value
        elif hasattr(entry, 'summary'):
            return entry.summary
        elif hasattr(entry, 'description'):
            return entry.description
        else:
            return entry.get('title', '')
    
    def _extract_date(self, entry) -> datetime:
        """ê²Œì‹œë¬¼ ë‚ ì§œ ì¶”ì¶œ"""
        date_fields = ['published', 'updated', 'created']
        
        for field in date_fields:
            if hasattr(entry, field):
                try:
                    date_str = getattr(entry, field)
                    parsed_date = date_parser.parse(date_str)
                    
                    # timezone ì œê±°
                    if parsed_date.tzinfo:
                        parsed_date = parsed_date.replace(tzinfo=None)
                    
                    return parsed_date
                except Exception as e:
                    continue
        
        # ë‚ ì§œ ì—†ìœ¼ë©´ None
        return None